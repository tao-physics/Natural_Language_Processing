{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'RT O\\n'\n",
      "'@TheValarium O\\n'\n",
      "': O\\n'\n",
      "'Online O\\n'\n",
      "'ticket O\\n'\n",
      "'sales O\\n'\n",
      "'for O\\n'\n",
      "'Ghostland B-musicartist\\n'\n",
      "'Observatory I-musicartist\\n'\n",
      "'extended O\\n'\n"
     ]
    }
   ],
   "source": [
    "with open('data/train.txt', encoding='utf-8') as f:\n",
    "    for _ in range(10):\n",
    "        line = f.readline()\n",
    "        print(repr(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dylan B-person\n",
      "\n",
      "refuses O\n",
      "\n",
      "to O\n",
      "\n",
      "take O\n",
      "\n",
      "a O\n",
      "\n",
      "nap O\n",
      "\n",
      "and O\n",
      "\n",
      "he O\n",
      "\n",
      "was O\n",
      "\n",
      "up O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/validation.txt', encoding='utf-8') as f:\n",
    "    for _ in range(10):\n",
    "        line = f.readline()\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man O\n",
      "\n",
      "i O\n",
      "\n",
      "hate O\n",
      "\n",
      "when O\n",
      "\n",
      "people O\n",
      "\n",
      "carry O\n",
      "\n",
      "ragedy O\n",
      "\n",
      "luggage O\n",
      "\n",
      ".. O\n",
      "\n",
      "ima O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/test.txt', encoding='utf-8') as f:\n",
    "    for _ in range(10):\n",
    "        line = f.readline()\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    tweet_tokens = []\n",
    "    tweet_tags = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tweet_tokens:\n",
    "                tokens.append(tweet_tokens)\n",
    "                tags.append(tweet_tags)\n",
    "            tweet_tokens = []\n",
    "            tweet_tags = []\n",
    "        else:\n",
    "            token, tag = line.split()\n",
    "            # Replace all urls with <URL> token\n",
    "            if token.lower().startswith('http://') or token.lower().startswith('https://'):\n",
    "                token = '<URL>'\n",
    "            \n",
    "            # Replace all users with <USR> token\n",
    "            if token.startswith('@'):\n",
    "                token = '<USR>'\n",
    "            \n",
    "            tweet_tokens.append(token)\n",
    "            tweet_tags.append(tag)\n",
    "            \n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_tags = read_data('data/train.txt')\n",
    "validation_tokens, validation_tags = read_data('data/validation.txt')\n",
    "test_tokens, test_tags = read_data('data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT\tO\n",
      "<USR>\tO\n",
      ":\tO\n",
      "Online\tO\n",
      "ticket\tO\n",
      "sales\tO\n",
      "for\tO\n",
      "Ghostland\tB-musicartist\n",
      "Observatory\tI-musicartist\n",
      "extended\tO\n",
      "until\tO\n",
      "6\tO\n",
      "PM\tO\n",
      "EST\tO\n",
      "due\tO\n",
      "to\tO\n",
      "high\tO\n",
      "demand\tO\n",
      ".\tO\n",
      "Get\tO\n",
      "them\tO\n",
      "before\tO\n",
      "they\tO\n",
      "sell\tO\n",
      "out\tO\n",
      "...\tO\n",
      "\n",
      "Apple\tB-product\n",
      "MacBook\tI-product\n",
      "Pro\tI-product\n",
      "A1278\tI-product\n",
      "13.3\tI-product\n",
      "\"\tI-product\n",
      "Laptop\tI-product\n",
      "-\tI-product\n",
      "MD101LL/A\tI-product\n",
      "(\tO\n",
      "June\tO\n",
      ",\tO\n",
      "2012\tO\n",
      ")\tO\n",
      "-\tO\n",
      "Full\tO\n",
      "read\tO\n",
      "by\tO\n",
      "eBay\tB-company\n",
      "<URL>\tO\n",
      "<URL>\tO\n",
      "\n",
      "Happy\tO\n",
      "Birthday\tO\n",
      "<USR>\tO\n",
      "!\tO\n",
      "May\tO\n",
      "Allah\tB-person\n",
      "s.w.t\tO\n",
      "bless\tO\n",
      "you\tO\n",
      "with\tO\n",
      "goodness\tO\n",
      "and\tO\n",
      "happiness\tO\n",
      ".\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
    "        print('%s\\t%s' % (token, tag))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(tokens_or_tags, special_tokens):\n",
    "    \"\"\"\n",
    "        tokens_or_tags: a list of lists of tokens or tags\n",
    "        special_tokens: some special tokens\n",
    "    \"\"\"\n",
    "    # Create a dictionary with default value 0\n",
    "    tok2idx = defaultdict(lambda: 0)\n",
    "    idx2tok = []\n",
    "    \n",
    "    # Create mappings from tokens (or tags) to indices and vice versa.\n",
    "    # At first, add special tokens (or tags) to the dictionaries.\n",
    "    # The first special token must have index 0.\n",
    "\n",
    "    # idx = 0\n",
    "    # for s_token in special_tokens:\n",
    "    #   tok2idx[s_token] = idx + 1\n",
    "    #   idx += 1\n",
    "    \n",
    "    # Mapping tok2idx should contain each token or tag only once. \n",
    "    # To do so, you should:\n",
    "    # 1. extract unique tokens/tags from the tokens_or_tags variable, which is not\n",
    "    #    occur in special_tokens (because they could have non-empty intersection)\n",
    "    # 2. index them (for example, you can add them into the list idx2tok\n",
    "    # 3. for each token/tag save the index into tok2idx).\n",
    "    \n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    uni_toks = list(set([tok for tweet in tokens_or_tags for tok in tweet] + special_tokens))\n",
    "    vocab = uni_toks\n",
    "  \n",
    "    for i,tok in enumerate(vocab):\n",
    "        tok2idx[tok] = i\n",
    "        idx2tok.append(tok)\n",
    "    \n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The unknown words will just show up while you transfer them into sequences with the dictionary.\n",
    "These unknown words will be directed to '<UNK>'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<UNK>', '<PAD>']\n",
    "special_tags = ['O']\n",
    "\n",
    "# Create dictionaries \n",
    "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
    "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-other',\n",
       " 'I-other',\n",
       " 'I-company',\n",
       " 'I-movie',\n",
       " 'I-tvshow',\n",
       " 'B-person',\n",
       " 'I-geo-loc',\n",
       " 'B-company',\n",
       " 'I-person',\n",
       " 'B-facility',\n",
       " 'B-product',\n",
       " 'B-musicartist',\n",
       " 'B-tvshow',\n",
       " 'B-movie',\n",
       " 'B-sportsteam',\n",
       " 'I-sportsteam',\n",
       " 'B-geo-loc',\n",
       " 'I-musicartist',\n",
       " 'I-product',\n",
       " 'I-facility']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2idxs(tokens_list):\n",
    "    return [token2idx[word] for word in tokens_list]\n",
    "\n",
    "def tags2idxs(tags_list):\n",
    "    return [tag2idx[tag] for tag in tags_list]\n",
    "\n",
    "def idxs2words(idxs):\n",
    "    return [idx2token[idx] for idx in idxs]\n",
    "\n",
    "def idxs2tags(idxs):\n",
    "    return [idx2tag[idx] for idx in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_vec = [words2idxs(words) for words in train_tokens]\n",
    "train_tokens_len = [len(words) for words in train_tokens]\n",
    "train_tags_vec = [tags2idxs(tags) for tags in train_tags]\n",
    "\n",
    "validation_tokens_vec = [words2idxs(words) for words in validation_tokens]\n",
    "validation_tokens_len = [len(words) for words in validation_tokens]\n",
    "validation_tags_vec = [tags2idxs(tags) for tags in validation_tags]\n",
    "\n",
    "test_tokens_vec = [words2idxs(words) for words in test_tokens]\n",
    "test_tags_vec = [tags2idxs(tags) for tags in test_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 12,\n",
       " 18,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tags_vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 37)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.amax(train_tokens_len), np.amax(validation_tokens_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(idx2token)\n",
    "embedding_dim = 100 \n",
    "max_length = 50\n",
    "padding_type = 'post'\n",
    "trunc_type = 'post'\n",
    "\n",
    "\n",
    "train_tokens_seq = pad_sequences(sequences = train_tokens_vec,\n",
    "             maxlen = max_length,\n",
    "             padding = padding_type,\n",
    "             truncating = trunc_type,\n",
    "             value = token2idx['<PAD>'])\n",
    "\n",
    "train_tags_seq = pad_sequences(sequences = train_tags_vec,\n",
    "             maxlen = max_length,\n",
    "             padding = padding_type,\n",
    "             truncating = trunc_type,\n",
    "             value = tag2idx['O'])\n",
    "\n",
    "validation_tokens_seq = pad_sequences(sequences = validation_tokens_vec,\n",
    "             maxlen = max_length,\n",
    "             padding = padding_type,\n",
    "             truncating = trunc_type,\n",
    "             value = token2idx['<PAD>'])\n",
    "\n",
    "validation_tags_seq = pad_sequences(sequences = validation_tags_vec,\n",
    "             maxlen = max_length,\n",
    "             padding = padding_type,\n",
    "             truncating = trunc_type,\n",
    "             value = tag2idx['O'])\n",
    "\n",
    "test_tokens_seq = pad_sequences(sequences = test_tokens_vec,\n",
    "             maxlen = max_length,\n",
    "             padding = padding_type,\n",
    "             truncating = trunc_type,\n",
    "             value = token2idx['<PAD>'])\n",
    "\n",
    "test_tags_seq = pad_sequences(sequences = test_tags_vec,\n",
    "             maxlen = max_length,\n",
    "             padding = padding_type,\n",
    "             truncating = trunc_type,\n",
    "             value = tag2idx['O'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(724, 50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tags_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35173,   103,    93,    40,    10,     5,   104,    52,    84,\n",
       "          66,    47,    28,    27,     7,     8,    31,    12,   165,\n",
       "          24,    60,    61], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test_tags_seq, return_counts = True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9716298342541436"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "35173 / np.unique(test_tags_seq, return_counts = True)[1].sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "~0.972 is the accuracy if one just predicts 'O' for all the tokens, which means no tag is identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove.840B.300d.txt\n",
    "# glove.6B.100d.txt\n",
    "\n",
    "glove_file = 'data/glove.6B.100d.txt'\n",
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf-8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "       \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'.\\xa0.\\xa0.'.replace('\\xa0', ' ') == '. . .'  # \\xa0 is used in GloVe dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68%\n"
     ]
    }
   ],
   "source": [
    "test_word = idx2token[0]\n",
    "print(test_word)\n",
    "\n",
    "# word_to_vec_map[test_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(idx2token)\n",
    "embedding_dim = 100 \n",
    "\n",
    "def build_matrix(index_word):\n",
    "    embedding_matrix = np.zeros((vocab_size, 100))\n",
    "    for i, word in enumerate(index_word):\n",
    "        try:\n",
    "            embedding_matrix[i] = word_to_vec_map[word]\n",
    "        except KeyError:\n",
    "            try:\n",
    "                embedding_matrix[i] = word_to_vec_map[word.lower()]\n",
    "            except:\n",
    "                try:\n",
    "                    embedding_matrix[i] = word_to_vec_map[word.title()]\n",
    "                except:\n",
    "                    pass\n",
    "    return embedding_matrix\n",
    "                \n",
    "                \n",
    "        \n",
    "embedding_matrix = build_matrix(idx2token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.35876  ,  0.58159  , -0.027631 , -0.25278  , -0.62805  ,\n",
       "       -0.42516  ,  0.24538  ,  0.60198  ,  0.16226  , -0.043216 ,\n",
       "        0.12896  , -0.045218 ,  0.43728  ,  0.093024 , -0.44829  ,\n",
       "       -0.36199  ,  0.1906   ,  0.48196  , -0.59453  , -0.36116  ,\n",
       "       -0.12156  , -0.16411  ,  0.057408 , -0.45872  , -0.096812 ,\n",
       "        0.46578  , -0.15785  , -0.097296 ,  0.75225  ,  0.072599 ,\n",
       "       -0.71558  ,  0.29924  , -0.15847  ,  0.21901  ,  0.98759  ,\n",
       "        0.43268  ,  0.026921 , -0.28457  ,  0.092205 , -0.30228  ,\n",
       "       -0.111    , -0.055727 ,  0.56083  , -0.85266  , -0.07291  ,\n",
       "       -0.18132  , -0.25805  , -0.25662  ,  0.084227 , -1.1677   ,\n",
       "        0.0035248, -0.018172 ,  0.31162  ,  0.73281  , -0.14573  ,\n",
       "       -2.0167   ,  0.043788 , -0.21965  ,  1.6191   ,  0.22962  ,\n",
       "        0.061044 ,  0.72638  , -0.97811  , -0.07765  ,  0.78095  ,\n",
       "        0.086923 ,  0.10981  ,  0.72691  , -0.88268  , -0.44801  ,\n",
       "        0.85117  , -0.84212  , -0.61164  , -0.67277  ,  0.21992  ,\n",
       "        0.39902  , -0.27581  ,  0.3659   , -0.011984 , -0.14905  ,\n",
       "        0.75221  , -0.30608  , -0.5833   ,  0.30957  , -0.67966  ,\n",
       "       -0.37554  ,  0.44417  ,  0.52     , -0.39893  , -0.25307  ,\n",
       "        0.23551  ,  0.30843  ,  0.026624 , -0.44868  , -0.76576  ,\n",
       "        0.041229 ,  0.13629  ,  0.077413 ,  0.54362  ,  0.51691  ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Flatten #Dense, LSTM, Input ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(idx2token)\n",
    "n_tags = len(idx2tag)\n",
    "embedding_dim = 100 \n",
    "max_length = 50\n",
    "\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(input_dim = vocab_size,\n",
    "                            output_dim = embedding_dim,\n",
    "                            input_length = max_length,\n",
    "                            weights = [embedding_matrix],\n",
    "                            trainable = False\n",
    "                           )\n",
    "\n",
    "model_input = Input(shape=(max_length, ))\n",
    "\n",
    "x = embedding_layer(model_input)\n",
    "\n",
    "x_LSTM1 = Bidirectional(LSTM(units=128, \n",
    "                           return_sequences=True, \n",
    "                           recurrent_dropout=0.2,\n",
    "                           dropout=0.2))(x)\n",
    "\n",
    "x_LSTM2 = Bidirectional(LSTM(units=128,\n",
    "                             return_sequences=True,\n",
    "                             recurrent_dropout=0.2,\n",
    "                             dropout=0.2))(x_LSTM1)\n",
    "\n",
    "\n",
    "\n",
    "x = Dense(128, activation='relu')(x_LSTM2)\n",
    "\n",
    "# x = layers.concatenate([x_LSTM1, x_LSTM2])\n",
    "\n",
    "model_output = Dense(n_tags, activation='softmax')(x)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs = model_input, outputs = model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 50, 100)           2050500   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 50, 256)           234496    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 256)           394240    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50, 128)           32896     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50, 21)            2709      \n",
      "=================================================================\n",
      "Total params: 2,714,841\n",
      "Trainable params: 664,341\n",
      "Non-trainable params: 2,050,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5795 samples, validate on 724 samples\n",
      "Epoch 1/5\n",
      "5795/5795 [==============================] - 41s 7ms/sample - loss: 0.2397 - accuracy: 0.9681 - val_loss: 0.1258 - val_accuracy: 0.9754\n",
      "Epoch 2/5\n",
      "5795/5795 [==============================] - 36s 6ms/sample - loss: 0.1316 - accuracy: 0.9741 - val_loss: 0.1108 - val_accuracy: 0.9768\n",
      "Epoch 3/5\n",
      "5795/5795 [==============================] - 36s 6ms/sample - loss: 0.1151 - accuracy: 0.9758 - val_loss: 0.0947 - val_accuracy: 0.9790\n",
      "Epoch 4/5\n",
      "5795/5795 [==============================] - 35s 6ms/sample - loss: 0.1019 - accuracy: 0.9777 - val_loss: 0.0853 - val_accuracy: 0.9809\n",
      "Epoch 5/5\n",
      "5795/5795 [==============================] - 35s 6ms/sample - loss: 0.0938 - accuracy: 0.9786 - val_loss: 0.0796 - val_accuracy: 0.9816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x193b4ca94a8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "          x = train_tokens_seq,\n",
    "          y = train_tags_seq,\n",
    "#          batch_size = 64,\n",
    "         epochs = 5,\n",
    "         verbose = 1,\n",
    "         #validation_split=0.1\n",
    "         validation_data = (validation_tokens_seq, validation_tags_seq)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1051552983114074, 0.9772652]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x = test_tokens_seq, \n",
    "               y = test_tags_seq,\n",
    "              batch_size = None,\n",
    "              verbose = 0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Accuracy is ~0.977, which is slightly better than the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_one = model.predict(x = test_tokens_seq[0:1],\n",
    "             batch_size = None,\n",
    "             verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50, 21)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_one.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9236101e-01, 1.0988724e-03, 6.6390930e-04, ..., 4.5722470e-04,\n",
       "        3.9113776e-04, 1.5672176e-04],\n",
       "       [9.9878329e-01, 1.0728989e-04, 2.6566739e-04, ..., 1.0351578e-04,\n",
       "        7.4374824e-05, 4.1915428e-05],\n",
       "       [9.9859470e-01, 3.3697265e-04, 1.3910639e-04, ..., 4.5106139e-05,\n",
       "        3.2994820e-05, 2.0195446e-05],\n",
       "       ...,\n",
       "       [9.9996245e-01, 2.6246739e-06, 6.5932488e-07, ..., 8.3189803e-07,\n",
       "        4.9715044e-07, 1.5572590e-07],\n",
       "       [9.9993718e-01, 3.9095648e-06, 1.0917832e-06, ..., 1.5497109e-06,\n",
       "        8.9663178e-07, 3.4193113e-07],\n",
       "       [9.9979728e-01, 1.0017093e-05, 3.1354207e-06, ..., 5.3247836e-06,\n",
       "        2.9472085e-06, 1.5884386e-06]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_one[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 21)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_one[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_pred = model.predict(x = test_tokens_seq,\n",
    "             batch_size = None,\n",
    "             verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(test_pred[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_matrix = np.zeros((724, 50), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(724, 50)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_id in range(724):\n",
    "    results_matrix[row_id] = np.argmax(test_pred[row_id], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0, 17, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 17,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0, 17,  0, 17,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 17,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_matrix[5:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0, 10, 20, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  8,  0,  0,  0,  0,  0,  0,  0, 17,  0, 17,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  1,  2,  2,  6,  9,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 6,  9,  9,  9,  0,  0,  0, 17,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tags_seq[5:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'O'),\n",
       " (1, 'B-other'),\n",
       " (2, 'I-other'),\n",
       " (3, 'I-company'),\n",
       " (4, 'I-movie'),\n",
       " (5, 'I-tvshow'),\n",
       " (6, 'B-person'),\n",
       " (7, 'I-geo-loc'),\n",
       " (8, 'B-company'),\n",
       " (9, 'I-person'),\n",
       " (10, 'B-facility'),\n",
       " (11, 'B-product'),\n",
       " (12, 'B-musicartist'),\n",
       " (13, 'B-tvshow'),\n",
       " (14, 'B-movie'),\n",
       " (15, 'B-sportsteam'),\n",
       " (16, 'I-sportsteam'),\n",
       " (17, 'B-geo-loc'),\n",
       " (18, 'I-musicartist'),\n",
       " (19, 'I-product'),\n",
       " (20, 'I-facility')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, tag) for i,tag in enumerate(idx2tag)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
